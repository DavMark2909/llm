{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Database connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyprojroot import here\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "from langchain_community.utilities import SQLDatabase\n",
    "\n",
    "database_path = here(\"data/sqldb.db\")\n",
    "\n",
    "connection_string = f\"sqlite:///{database_path}\"\n",
    "engine = create_engine(connection_string, echo=True)\n",
    "db = SQLDatabase(engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connection to the desired LLM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "  os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "llm = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing a promt engineering by extracting a desitred prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mark/Documents/Research/LLM_MODEL/.venv/lib/python3.11/site-packages/langsmith/client.py:253: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain import hub\n",
    "\n",
    "query_prompt_template = hub.pull(\"langchain-ai/sql-query-system-prompt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The message that should be passed to the model in case of teh error while running the query against the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['dialect', 'input', 'table_info', 'top_k'] input_types={} partial_variables={} metadata={'lc_hub_owner': 'langchain-ai', 'lc_hub_repo': 'sql-query-system-prompt', 'lc_hub_commit_hash': '5d6c20e97a0a3dc6f955719a185eb8987d9fce8a04ec1df70344ff92497ebcfb'} messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['dialect', 'input', 'table_info', 'top_k'], input_types={}, partial_variables={}, template='Given an input question, create a syntactically correct {dialect} query to run to help find the answer. Unless the user specifies in his question a specific number of examples they wish to obtain, always limit your query to at most {top_k} results. You can order the results by a relevant column to return the most interesting examples in the database.\\n\\nNever query for all the columns from a specific table, only ask for a the few relevant columns given the question.\\n\\nPay attention to use only the column names that you can see in the schema description. Be careful to not query for columns that do not exist. Also, pay attention to which column is in which table.\\n\\nOnly use the following tables:\\n{table_info}\\n\\nQuestion: {input}'), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "error_template = PromptTemplate(\n",
    "    input_variables=[\"dialect\", \"input\", \"table_info\", \"top_k\", \"error\", \"prev_result\"],\n",
    "    template=(\n",
    "        \"Given an input question, create a syntactically correct {dialect} query to run to help find the answer. Unless the user specifies in his question a specific number of examples they wish to obtain, always limit your query to at most {top_k} results.\"\n",
    "        \"You can order the results by a relevant column to return the most interesting examples in the database.\" \n",
    "        \"Never query for all the columns from a specific table, only ask for a the few relevant columns given the question.\"\n",
    "        \"Pay attention to use only the column names that you can see in the schema description. Be careful to not query for columns that do not exist. Also, pay attention to which column is in which table.\"\n",
    "        \"Only use the following tables:\\n{table_info}\"\n",
    "        \"Question: {input}\"\n",
    "        \"Also, you have already been asked this question and you produced the next result: {error_result}.\"\n",
    "        \"However, it generated the next error while trying to execute: {error}\"\n",
    "        \"Based on the question and the previously generated error, please try to fix the query or gengerate a new one\"\n",
    "    )\n",
    ")\n",
    "\n",
    "sys_msg = SystemMessagePromptTemplate(prompt=error_template)\n",
    "error_prompt_template = ChatPromptTemplate([sys_msg])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a state for messaging tracking among the tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict, List\n",
    "\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    query: str\n",
    "    result: str\n",
    "    answer: str\n",
    "    error: str\n",
    "    counter: int\n",
    "    tables: List[str]\n",
    "    prev_result: str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a set of tools that are responsible for the selection of the most relevant tables for a particular request and the eventual generation of the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import Annotated\n",
    "from langchain.tools import tool\n",
    "import yaml\n",
    "\n",
    "\n",
    "class QueryOutput(TypedDict):\n",
    "    \"\"\"Generated SQL query.\"\"\"\n",
    "\n",
    "    query: Annotated[str, ..., \"Syntactically valid SQL query.\"]\n",
    "\n",
    "\n",
    "@tool\n",
    "def handle_request(user_input: str) -> State:\n",
    "    return {\"question\": user_input, \"query\": \"\", \"result\": \"\", \"answer\": \"\", \"tables\": []}\n",
    "\n",
    "@tool\n",
    "def select_tables(state: State):\n",
    "    question = state[\"question\"]\n",
    "    with open('../config/config.yml', 'r') as file:\n",
    "        config = yaml.safe_load(file)\n",
    "    llm_config = config.get(\"llm_model\")\n",
    "    prompt = llm_config.get(\"table_prompt\")\n",
    "    tables = db.get_table_names()\n",
    "    filled_prompt = prompt.format(question=question, table_names=tables)\n",
    "    result = llm.predict(filled_prompt)\n",
    "    return {\"tables\": result.split(\",\")}\n",
    "\n",
    "@tool\n",
    "def confirm_table_selection(state: State):\n",
    "    tables = state[\"tables\"]\n",
    "    confirm = input(f\"Are the next table correct? {tables} (yes/no)\")\n",
    "    if confirm.lower() == \"yes\":\n",
    "        return {\"tables\": tables}\n",
    "    # resolve the no branch\n",
    "\n",
    "@tool\n",
    "def schema_retriever_query_generator(state: State):\n",
    "    tables = state[\"tables\"]\n",
    "    schemas = db.get_table_info(tables)\n",
    "    \"\"\"Generate SQL query to fetch information.\"\"\"\n",
    "\n",
    "    if \"error\" in state:\n",
    "        prompt = error_prompt_template.invoke(\n",
    "            {\n",
    "                \"dialect\": db.dialect,\n",
    "                \"input\": state[\"question\"],\n",
    "                \"table_info\": schemas,\n",
    "                \"top_k\": 10,\n",
    "                \"error\": state[\"error\"],\n",
    "                \"prev_result\": state[\"prev_result\"]\n",
    "            }\n",
    "        )\n",
    "    else:\n",
    "        prompt = query_prompt_template.invoke(\n",
    "            {\n",
    "                \"dialect\": db.dialect,\n",
    "                \"top_k\": 10,\n",
    "                \"table_info\": schemas,\n",
    "                \"input\": state[\"question\"],\n",
    "            }\n",
    "        )\n",
    "\n",
    "    structured_llm = llm.with_structured_output(QueryOutput)\n",
    "    result = structured_llm.invoke(prompt)\n",
    "    return {\"query\": result[\"query\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Block of code responsible for running the generated query against the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.sql_database.tool import QuerySQLDatabaseTool\n",
    "\n",
    "execute_query_tool = QuerySQLDatabaseTool(db=db)\n",
    "\n",
    "@tool\n",
    "def execute_query(state: State):\n",
    "    \"\"\"Execute SQL query.\"\"\"\n",
    "    try:\n",
    "        result = execute_query_tool.invoke(state[\"query\"])\n",
    "        return {\"result\": result}\n",
    "    except Exception as e:\n",
    "        err_msg = str(e)\n",
    "        state[\"error\"] = err_msg\n",
    "        state[\"prev_result\"] = state[\"query\"]\n",
    "        state[\"counter\"] = state[\"counter\"] + 1\n",
    "        \n",
    "    # return {\"result\": execute_query_tool.invoke(state[\"query\"])}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
